{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ¦º Intelligent PPE Monitoring System â€” Colab Test\n",
                "\n",
                "**Purpose:** Test the full YOLO + SAM hybrid detection pipeline on a free GPU before Azure deployment.\n",
                "\n",
                "## Pipeline Overview\n",
                "```\n",
                "Image â†’ YOLO (1280px) â†’ 5-Path Decision â†’ SAM (ROI crop) â†’ Result\n",
                "         ~35ms                              ~1500ms async\n",
                "```\n",
                "\n",
                "## Before Running\n",
                "1. **Runtime â†’ Change runtime type â†’ T4 GPU** âœ…\n",
                "2. Upload your models to Google Drive:\n",
                "   - `MyDrive/ppe_models/best.pt` (your YOLO model)\n",
                "   - `MyDrive/ppe_models/sam3.pt` (SAM model)\n",
                "3. Run cells **top to bottom** in order\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Verify GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "print('='*50)\n",
                "print('GPU CHECK')\n",
                "print('='*50)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "    print(f'âœ… GPU: {gpu_name}')\n",
                "    print(f'âœ… VRAM: {gpu_mem:.1f} GB')\n",
                "else:\n",
                "    print('âŒ No GPU found!')\n",
                "    print('   Go to Runtime â†’ Change runtime type â†’ T4 GPU')\n",
                "    raise RuntimeError('GPU required for SAM. Please enable GPU runtime.')\n",
                "\n",
                "print(f'âœ… PyTorch: {torch.__version__}')\n",
                "print(f'âœ… CUDA: {torch.version.cuda}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Installing dependencies...')\n",
                "\n",
                "# Core ML\n",
                "!pip install -q ultralytics>=8.0.0\n",
                "\n",
                "# API server + tunnel\n",
                "!pip install -q fastapi uvicorn[standard] python-multipart pyngrok\n",
                "\n",
                "# Database + config\n",
                "!pip install -q sqlalchemy pydantic>=2.0.0 pydantic-settings python-dotenv\n",
                "\n",
                "# Image processing\n",
                "!pip install -q opencv-python-headless Pillow\n",
                "\n",
                "# Reporting\n",
                "!pip install -q reportlab apscheduler\n",
                "\n",
                "# Download Sam 3\n",
                "#!wget --header=\"Authorization: Bearer YOUR_HF_TOKEN\" \"https://huggingface.co/facebook/sam3/resolve/main/sam3.pt\"\n",
                "\n",
                "print('\\nâœ… All dependencies installed!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Mount Google Drive & Copy Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os, shutil\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# â”€â”€ Configure your Drive paths here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "DRIVE_YOLO_MODEL = '/content/drive/MyDrive/ppe_models/best.pt'\n",
                "DRIVE_SAM_MODEL  = '/content/drive/MyDrive/ppe_models/sam3.pt'\n",
                "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "# Create models directory\n",
                "os.makedirs('/content/backend/models', exist_ok=True)\n",
                "\n",
                "# Copy models (faster than loading from Drive every time)\n",
                "print('Copying models from Drive...')\n",
                "\n",
                "if os.path.exists(DRIVE_YOLO_MODEL):\n",
                "    shutil.copy(DRIVE_YOLO_MODEL, '/content/backend/models/best.pt')\n",
                "    size_mb = os.path.getsize('/content/backend/models/best.pt') / 1e6\n",
                "    print(f'âœ… YOLO model copied ({size_mb:.1f} MB)')\n",
                "else:\n",
                "    print(f'âŒ YOLO model not found at: {DRIVE_YOLO_MODEL}')\n",
                "    print('   Please upload best.pt to your Google Drive')\n",
                "\n",
                "if os.path.exists(DRIVE_SAM_MODEL):\n",
                "    shutil.copy(DRIVE_SAM_MODEL, '/content/backend/models/sam3.pt')\n",
                "    size_mb = os.path.getsize('/content/backend/models/sam3.pt') / 1e6\n",
                "    print(f'âœ… SAM model copied ({size_mb:.1f} MB)')\n",
                "else:\n",
                "    print(f'âš ï¸  SAM model not found at: {DRIVE_SAM_MODEL}')\n",
                "    print('   System will run in YOLO-only mode (SAM disabled)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Clone / Upload Backend Code\n",
                "\n",
                "**Option A:** Clone from GitHub (if you pushed your code)\n",
                "\n",
                "**Option B:** Upload the backend folder as a zip (manual)\n",
                "\n",
                "Run whichever applies to you:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# â”€â”€ OPTION A: Clone from GitHub â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# Uncomment and fill in your repo URL:\n",
                "# !git clone https://github.com/YOUR_USERNAME/intelligent-ppe-monitoring.git /content/repo\n",
                "# !cp -r /content/repo/backend/* /content/backend/\n",
                "\n",
                "# â”€â”€ OPTION B: Copy from Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# If you zipped your backend folder and uploaded to Drive:\n",
                "# DRIVE_BACKEND_ZIP = '/content/drive/MyDrive/ppe_models/backend.zip'\n",
                "# !unzip -q {DRIVE_BACKEND_ZIP} -d /content/backend_extracted\n",
                "# !cp -r /content/backend_extracted/backend/* /content/backend/\n",
                "\n",
                "# â”€â”€ OPTION C: Write backend inline (auto-setup) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# This creates the minimum files needed to run detection\n",
                "# Use this if you don't have GitHub or Drive zip\n",
                "\n",
                "print('Choose your option above and uncomment the relevant lines.')\n",
                "print('Then re-run this cell.')\n",
                "print()\n",
                "print('If using Option C (inline), run the next cell instead.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4b: Auto-Setup Backend (if no GitHub/Drive zip)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Create full directory structure\n",
                "dirs = [\n",
                "    '/content/backend',\n",
                "    '/content/backend/config',\n",
                "    '/content/backend/database',\n",
                "    '/content/backend/services',\n",
                "    '/content/backend/agents',\n",
                "    '/content/backend/api',\n",
                "    '/content/backend/api/routes',\n",
                "    '/content/backend/api/models',\n",
                "    '/content/backend/utils',\n",
                "    '/content/backend/models',\n",
                "    '/content/backend/uploads',\n",
                "    '/content/backend/reports',\n",
                "]\n",
                "for d in dirs:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "# Create __init__.py files\n",
                "init_dirs = ['config','database','services','agents','api','api/routes','api/models','utils']\n",
                "for d in init_dirs:\n",
                "    open(f'/content/backend/{d}/__init__.py', 'w').close()\n",
                "\n",
                "print('âœ… Directory structure created')\n",
                "print()\n",
                "print('âš ï¸  You still need to copy your Python source files.')\n",
                "print('   Best approach: zip your backend folder, upload to Drive,')\n",
                "print('   then use Option B in Cell 4.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Configure Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Write .env file for the backend\n",
                "env_content = \"\"\"\n",
                "# Database\n",
                "DATABASE_URL=sqlite:////content/backend/ppe_detection.db\n",
                "\n",
                "# YOLO Model\n",
                "YOLO_MODEL_PATH=/content/backend/models/best.pt\n",
                "YOLO_CONFIDENCE_THRESHOLD=0.25\n",
                "YOLO_IMGSZ=1280\n",
                "\n",
                "# SAM Model\n",
                "SAM_ENABLED=true\n",
                "SAM_MODEL_PATH=/content/backend/models/sam3.pt\n",
                "SAM_DEVICE=cuda\n",
                "SAM_MASK_THRESHOLD=0.05\n",
                "\n",
                "# Violation Tracking\n",
                "VIOLATION_COOLDOWN_SECONDS=300\n",
                "VIOLATION_IOU_THRESHOLD=0.3\n",
                "VIOLATION_TRACK_TIMEOUT=60\n",
                "\n",
                "# Site defaults\n",
                "DEFAULT_SITE_LOCATION=Colab Test Site\n",
                "DEFAULT_CAMERA_ID=colab_cam_01\n",
                "\n",
                "# Reporting\n",
                "REPORT_OUTPUT_DIR=/content/backend/reports\n",
                "REPORT_TIME=23:59\n",
                "\n",
                "# Debug\n",
                "DEBUG=true\n",
                "\"\"\".strip()\n",
                "\n",
                "with open('/content/backend/.env', 'w') as f:\n",
                "    f.write(env_content)\n",
                "\n",
                "print('âœ… .env file written')\n",
                "\n",
                "# Set working directory\n",
                "os.chdir('/content/backend')\n",
                "print(f'âœ… Working directory: {os.getcwd()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Initialize Database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '/content/backend')\n",
                "os.chdir('/content/backend')\n",
                "\n",
                "from database.connection import engine\n",
                "from database.models import Base\n",
                "\n",
                "# Create all tables\n",
                "Base.metadata.create_all(bind=engine)\n",
                "print('âœ… Database tables created')\n",
                "\n",
                "# List tables\n",
                "from sqlalchemy import inspect\n",
                "inspector = inspect(engine)\n",
                "tables = inspector.get_table_names()\n",
                "print(f'âœ… Tables: {tables}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: Load Models & Test Detection (No API)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os, time\n",
                "import numpy as np\n",
                "import cv2\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "sys.path.insert(0, '/content/backend')\n",
                "os.chdir('/content/backend')\n",
                "\n",
                "# â”€â”€ Load YOLO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print('Loading YOLO model...')\n",
                "from services.yolo_detector import get_yolo_detector\n",
                "yolo = get_yolo_detector()\n",
                "yolo.load_model()\n",
                "print('âœ… YOLO loaded')\n",
                "\n",
                "# â”€â”€ Load SAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print('Loading SAM model...')\n",
                "from services.sam_verifier import get_sam_verifier\n",
                "sam = get_sam_verifier()\n",
                "sam.load_model()\n",
                "if sam._use_mock:\n",
                "    print('âš ï¸  SAM running in MOCK mode (model not found or load failed)')\n",
                "else:\n",
                "    print('âœ… SAM loaded (real GPU inference)')\n",
                "\n",
                "# â”€â”€ Create test image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# Use a blank image for smoke test (replace with real image below)\n",
                "test_image = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
                "test_image[:] = (50, 50, 50)  # Dark gray background\n",
                "\n",
                "# â”€â”€ Run YOLO detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print('\\nRunning YOLO detection on test image...')\n",
                "t0 = time.time()\n",
                "yolo_result = yolo.detect(test_image)\n",
                "yolo_ms = (time.time() - t0) * 1000\n",
                "\n",
                "print(f'âœ… YOLO inference: {yolo_ms:.1f}ms')\n",
                "print(f'   Persons detected: {len(yolo_result[\"persons\"])}')\n",
                "print(f'   PPE items detected: {len(yolo_result[\"ppe_items\"])}')\n",
                "print()\n",
                "print('Smoke test passed! Ready to test with real images.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: Test with a Real Image\n",
                "\n",
                "Upload a construction site image and run the full hybrid detection pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import io\n",
                "\n",
                "print('Upload a test image (JPG/PNG)...')\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Load the uploaded image\n",
                "filename = list(uploaded.keys())[0]\n",
                "image_data = uploaded[filename]\n",
                "nparr = np.frombuffer(image_data, np.uint8)\n",
                "test_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
                "\n",
                "print(f'âœ… Image loaded: {filename} ({test_image.shape[1]}Ã—{test_image.shape[0]}px)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os\n",
                "sys.path.insert(0, '/content/backend')\n",
                "os.chdir('/content/backend')\n",
                "\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as patches\n",
                "\n",
                "# â”€â”€ Run full hybrid detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print('Running full hybrid detection pipeline...')\n",
                "print('='*50)\n",
                "\n",
                "from services.hybrid_detector import get_hybrid_detector\n",
                "detector = get_hybrid_detector()\n",
                "\n",
                "t0 = time.time()\n",
                "result = detector.detect_async(\n",
                "    test_image,\n",
                "    save_annotated=True,\n",
                "    output_path='/content/backend/uploads/test_annotated.jpg'\n",
                ")\n",
                "total_ms = (time.time() - t0) * 1000\n",
                "\n",
                "# â”€â”€ Print results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(f'\\nâ±ï¸  Total time: {total_ms:.1f}ms')\n",
                "print(f'   YOLO time:  {result[\"timing\"][\"yolo_ms\"]:.1f}ms')\n",
                "print(f'   SAM mode:   {result.get(\"sam_mode\", \"sync\")}')\n",
                "print(f'   SAM jobs:   {result.get(\"sam_jobs_pending\", 0)} pending in background')\n",
                "print()\n",
                "print(f'ğŸ‘¥ Persons detected: {result[\"stats\"][\"total_persons\"]}')\n",
                "print(f'ğŸš¨ Violations:       {result[\"stats\"][\"total_violations\"]}')\n",
                "print(f'âœ… Compliance rate:  {result[\"stats\"][\"compliance_rate\"]:.1f}%')\n",
                "print(f'ğŸ”¬ SAM activations:  {result[\"stats\"][\"sam_activations\"]}')\n",
                "print()\n",
                "\n",
                "for i, person in enumerate(result['persons']):\n",
                "    status = 'ğŸš¨ VIOLATION' if person['is_violation'] else 'âœ… SAFE'\n",
                "    print(f'  Person {i+1}: {status}')\n",
                "    print(f'    Helmet: {\"âœ…\" if person[\"has_helmet\"] else \"âŒ\"}')\n",
                "    print(f'    Vest:   {\"âœ…\" if person[\"has_vest\"] else \"âŒ\"}')\n",
                "    print(f'    Path:   {person[\"decision_path\"]}')\n",
                "    print(f'    SAM:    {\"Yes\" if person[\"sam_activated\"] else \"No\"}')\n",
                "    if person['violation_type']:\n",
                "        print(f'    Type:   {person[\"violation_type\"]}')\n",
                "    print()\n",
                "\n",
                "# â”€â”€ Visualize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
                "\n",
                "# Original image with bboxes\n",
                "img_rgb = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
                "axes[0].imshow(img_rgb)\n",
                "axes[0].set_title('Original + Detections', fontsize=14)\n",
                "\n",
                "for person in result['persons']:\n",
                "    x1, y1, x2, y2 = person['bbox']\n",
                "    color = 'red' if person['is_violation'] else 'green'\n",
                "    rect = patches.Rectangle(\n",
                "        (x1, y1), x2-x1, y2-y1,\n",
                "        linewidth=2, edgecolor=color, facecolor='none'\n",
                "    )\n",
                "    axes[0].add_patch(rect)\n",
                "    label = f'{person[\"decision_path\"]}'\n",
                "    axes[0].text(x1, y1-5, label, color=color, fontsize=9, fontweight='bold')\n",
                "\n",
                "axes[0].axis('off')\n",
                "\n",
                "# Annotated image (if saved)\n",
                "annotated_path = '/content/backend/uploads/test_annotated.jpg'\n",
                "if os.path.exists(annotated_path):\n",
                "    ann_img = cv2.imread(annotated_path)\n",
                "    ann_rgb = cv2.cvtColor(ann_img, cv2.COLOR_BGR2RGB)\n",
                "    axes[1].imshow(ann_rgb)\n",
                "    axes[1].set_title('Annotated Output', fontsize=14)\n",
                "else:\n",
                "    axes[1].text(0.5, 0.5, 'No annotated image', ha='center', va='center')\n",
                "axes[1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/detection_result.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('\\nâœ… Result saved to /content/detection_result.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Wait for SAM Results & Check Accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from services.async_sam_verifier import get_async_sam_verifier\n",
                "\n",
                "async_sam = get_async_sam_verifier()\n",
                "sam_jobs = result.get('sam_jobs', [])\n",
                "\n",
                "if not sam_jobs:\n",
                "    print('â„¹ï¸  No SAM jobs were submitted (all persons took fast paths)')\n",
                "else:\n",
                "    print(f'â³ Waiting for {len(sam_jobs)} SAM job(s) to complete...')\n",
                "    \n",
                "    for job_id in sam_jobs:\n",
                "        sam_result = async_sam.wait_for(job_id, timeout=60.0)\n",
                "        if sam_result:\n",
                "            print(f'\\nâœ… SAM Job {job_id[:8]}...')\n",
                "            print(f'   Helmet: {\"âœ…\" if sam_result.has_helmet else \"âŒ\"}')\n",
                "            print(f'   Vest:   {\"âœ…\" if sam_result.has_vest else \"âŒ\"}')\n",
                "            print(f'   Is violation: {sam_result.is_violation}')\n",
                "            print(f'   YOLO was correct: {sam_result.yolo_was_correct}')\n",
                "            print(f'   SAM latency: {sam_result.sam_latency_ms:.1f}ms')\n",
                "            if not sam_result.yolo_was_correct:\n",
                "                print(f'   ğŸ”„ SAM CORRECTED YOLO!')\n",
                "        else:\n",
                "            print(f'âš ï¸  Job {job_id[:8]}... timed out')\n",
                "\n",
                "# Print SAM statistics\n",
                "print('\\n' + '='*50)\n",
                "print('SAM STATISTICS (Thesis Metrics)')\n",
                "print('='*50)\n",
                "stats = async_sam.get_stats()\n",
                "for key, val in stats.items():\n",
                "    print(f'  {key}: {val}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Start FastAPI Server with ngrok\n",
                "\n",
                "This exposes your backend as a public URL so your frontend can connect to it.\n",
                "\n",
                "**Get a free ngrok token at:** https://dashboard.ngrok.com/signup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Set your ngrok token â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "NGROK_TOKEN = 'YOUR_NGROK_TOKEN_HERE'  # â† paste your token here\n",
                "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "import subprocess, threading, time\n",
                "from pyngrok import ngrok, conf\n",
                "\n",
                "# Configure ngrok\n",
                "conf.get_default().auth_token = NGROK_TOKEN\n",
                "\n",
                "# Kill any existing tunnels\n",
                "ngrok.kill()\n",
                "\n",
                "# Start FastAPI in background thread\n",
                "def run_server():\n",
                "    import subprocess\n",
                "    subprocess.run(\n",
                "        ['python', '-m', 'uvicorn', 'main:app', '--host', '0.0.0.0', '--port', '8000'],\n",
                "        cwd='/content/backend'\n",
                "    )\n",
                "\n",
                "server_thread = threading.Thread(target=run_server, daemon=True)\n",
                "server_thread.start()\n",
                "\n",
                "# Wait for server to start\n",
                "print('Starting FastAPI server...')\n",
                "time.sleep(8)\n",
                "\n",
                "# Create ngrok tunnel\n",
                "tunnel = ngrok.connect(8000)\n",
                "public_url = tunnel.public_url\n",
                "\n",
                "print('='*60)\n",
                "print('ğŸŒ PUBLIC API URL:')\n",
                "print(f'   {public_url}')\n",
                "print()\n",
                "print('ğŸ“‹ API Endpoints:')\n",
                "print(f'   Health:    {public_url}/api/health')\n",
                "print(f'   Detect:    {public_url}/api/detect  (POST)')\n",
                "print(f'   History:   {public_url}/api/history')\n",
                "print(f'   SAM Stats: {public_url}/api/sam/stats')\n",
                "print(f'   API Docs:  {public_url}/docs')\n",
                "print('='*60)\n",
                "print()\n",
                "print('ğŸ‘‰ Copy this URL into your frontend .env as VITE_API_URL')\n",
                "print(f'   VITE_API_URL={public_url}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 11: Test API via HTTP (while server is running)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests, json\n",
                "\n",
                "# Test health endpoint\n",
                "print('Testing /api/health...')\n",
                "r = requests.get(f'{public_url}/api/health')\n",
                "print(f'Status: {r.status_code}')\n",
                "print(json.dumps(r.json(), indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test detection endpoint with your uploaded image\n",
                "print('Testing /api/detect...')\n",
                "\n",
                "with open('/content/backend/uploads/test_annotated.jpg', 'rb') as f:\n",
                "    # Use original image if annotated doesn't exist\n",
                "    pass\n",
                "\n",
                "# Re-encode test image\n",
                "import cv2\n",
                "_, img_encoded = cv2.imencode('.jpg', test_image)\n",
                "\n",
                "r = requests.post(\n",
                "    f'{public_url}/api/detect',\n",
                "    files={'file': ('test.jpg', img_encoded.tobytes(), 'image/jpeg')},\n",
                "    data={'save_annotated': 'true'}\n",
                ")\n",
                "\n",
                "print(f'Status: {r.status_code}')\n",
                "result_api = r.json()\n",
                "print(f'Message: {result_api.get(\"message\")}')\n",
                "print(f'Persons: {result_api[\"stats\"][\"total_persons\"]}')\n",
                "print(f'Violations: {result_api[\"stats\"][\"total_violations\"]}')\n",
                "print(f'Compliance: {result_api[\"stats\"][\"compliance_rate\"]:.1f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 12: Benchmark â€” FPS & Latency Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from services.yolo_detector import get_yolo_detector\n",
                "\n",
                "yolo = get_yolo_detector()\n",
                "N_RUNS = 20\n",
                "\n",
                "print(f'Running {N_RUNS} inference passes for benchmarking...')\n",
                "\n",
                "latencies = []\n",
                "for i in range(N_RUNS):\n",
                "    t0 = time.time()\n",
                "    yolo.detect(test_image)\n",
                "    ms = (time.time() - t0) * 1000\n",
                "    latencies.append(ms)\n",
                "    if (i+1) % 5 == 0:\n",
                "        print(f'  Run {i+1}/{N_RUNS}: {ms:.1f}ms')\n",
                "\n",
                "# Skip first 3 warmup runs\n",
                "latencies_warm = latencies[3:]\n",
                "\n",
                "avg_ms = np.mean(latencies_warm)\n",
                "min_ms = np.min(latencies_warm)\n",
                "max_ms = np.max(latencies_warm)\n",
                "fps = 1000 / avg_ms\n",
                "\n",
                "print()\n",
                "print('='*50)\n",
                "print('YOLO BENCHMARK RESULTS (Thesis Table)')\n",
                "print('='*50)\n",
                "print(f'  Average latency: {avg_ms:.1f}ms')\n",
                "print(f'  Min latency:     {min_ms:.1f}ms')\n",
                "print(f'  Max latency:     {max_ms:.1f}ms')\n",
                "print(f'  FPS:             {fps:.1f}')\n",
                "print(f'  Image size:      1280Ã—1280')\n",
                "print(f'  Device:          {\"GPU\" if __import__(\"torch\").cuda.is_available() else \"CPU\"}')\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(latencies_warm, marker='o', linewidth=2, color='#2196F3')\n",
                "plt.axhline(avg_ms, color='red', linestyle='--', label=f'Avg: {avg_ms:.1f}ms')\n",
                "plt.xlabel('Run #')\n",
                "plt.ylabel('Latency (ms)')\n",
                "plt.title(f'YOLO Inference Latency â€” {fps:.1f} FPS')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/benchmark_yolo.png', dpi=150)\n",
                "plt.show()\n",
                "print('\\nâœ… Benchmark chart saved to /content/benchmark_yolo.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 13: Download Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Download detection result\n",
                "if os.path.exists('/content/detection_result.png'):\n",
                "    files.download('/content/detection_result.png')\n",
                "    print('âœ… Downloaded: detection_result.png')\n",
                "\n",
                "# Download benchmark chart\n",
                "if os.path.exists('/content/benchmark_yolo.png'):\n",
                "    files.download('/content/benchmark_yolo.png')\n",
                "    print('âœ… Downloaded: benchmark_yolo.png')\n",
                "\n",
                "# Download annotated image\n",
                "ann_path = '/content/backend/uploads/test_annotated.jpg'\n",
                "if os.path.exists(ann_path):\n",
                "    files.download(ann_path)\n",
                "    print('âœ… Downloaded: test_annotated.jpg')\n",
                "\n",
                "print('\\nğŸ“Š Use these images in your thesis!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## âœ… Checklist\n",
                "\n",
                "- [ ] GPU enabled (T4)\n",
                "- [ ] `best.pt` loaded successfully\n",
                "- [ ] `sam3.pt` loaded successfully (real, not mock)\n",
                "- [ ] YOLO detection works on test image\n",
                "- [ ] SAM async jobs complete\n",
                "- [ ] API server running with ngrok URL\n",
                "- [ ] Frontend can connect to ngrok URL\n",
                "- [ ] FPS benchmark recorded for thesis\n",
                "\n",
                "## ğŸ“ Thesis Metrics to Record\n",
                "\n",
                "| Metric | Value |\n",
                "|--------|-------|\n",
                "| YOLO inference time (ms) | __ |\n",
                "| YOLO FPS | __ |\n",
                "| SAM latency (ms) | __ |\n",
                "| False positives caught by SAM | __ |\n",
                "| False negatives caught by SAM | __ |\n",
                "| YOLO accuracy rate | __ % |\n",
                "| SAM activation rate | __ % |\n",
                "\n",
                "Fill these in from Cell 9 (SAM stats) and Cell 12 (benchmark)."
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "name": "PPE_Detection_System_Test.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
